# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TTB1OjqAfHYKBhmlRCqlU4AAa7CNLDJW
"""



import nltk
import random
import string
nltk.download('punkt') #to download nltk
import re   #regular expression(regex)
import urllib  #to handle url
import bs4 as bs #used for scrapping(traverse webpage/crawling)
from nltk.corpus import stopwords 

nltk.download('stopwords')

enter_input = input("Search: ")
u_i = string.capwords(enter_input)
address_list = u_i.split()  
address_keyword = "_".join(address_list)

url = "https://en.wikipedia.org/wiki/" + address_keyword

allwords = []  #words finally included in the result
linkToScrape = [] #list of all links on webpage
linkToScrape.append(url) 
for ul in linkToScrape:
    source = urllib.request.urlopen(ul)
    soup = bs.BeautifulSoup(source,'lxml')
    text = ""
    allLinks = soup.find_all('a' , href = True)
    random.shuffle(allLinks)
    
    for link in allLinks:
    	text += link.text
    	if link['href'].find("/wiki/") == -1: 
    		continue
    	if len(linkToScrape) < 500:
    	    linkToScrape.append(link['href'])
        
    text = re.sub(r'\[[0-9]*\]',' ', text)
    text = re.sub(r'\s+',' ',text)
    text = text.lower()
    text = re.sub(r'\d',' ', text)
    text = re.sub(r'\s+',' ',text)
    
    sentence = nltk.sent_tokenize(text)
    
    words = [nltk.word_tokenize(sent) for sent in sentence]
    allwords.append(words)
    
import pandas as pd

words = pd.DataFrame(allwords)
words.to_csv('words_mining.csv')